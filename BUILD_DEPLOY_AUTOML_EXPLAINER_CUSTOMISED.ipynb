{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AutoML: Train \"the best\" classifier model - With end-to-end MLFlow model experience.\n",
        "\n",
        "## Project Setup\n",
        "\n",
        "_**Make sure you use a compute instance that has the same sklearn version as the AutoML sklearn version.\n",
        "This is important when we work with fitting the explainer object.**_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Connect to Azure Machine Learning Workspace\n",
        "\n",
        "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
        "\n",
        "## 1.1. Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670894291630
        },
        "name": "automl-import"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.identity import AzureCliCredential\n",
        "from azure.ai.ml import automl, Input, MLClient\n",
        "import mltable\n",
        "\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml.automl import (\n",
        "    classification,\n",
        "    ClassificationPrimaryMetrics,\n",
        "    ClassificationModels,\n",
        ")\n",
        "\n",
        "# import required libraries\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        "    CodeConfiguration,\n",
        "    ProbeSettings,\n",
        ")\n",
        "from azure.ai.ml.constants import ModelType\n",
        "\n",
        "import shap \n",
        "import numpy as np\n",
        "import joblib\n",
        "import json\n",
        "import pandas as pd \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2. Workspace details\n",
        "\n",
        "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../configuration.ipynb) for more details on how to configure credentials and connect to a workspace.\n",
        "\n",
        "By default, we try to use the by default workspace configuration (available out-of-the-box in Compute Instances) or from any Config.json file you might have copied into the folders structure.\n",
        "If no Config.json is found, then you need to manually introduce the subscription_id, resource_group and workspace when creating MLClient ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670894292044
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "name": "mlclient-setup",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "credential = DefaultAzureCredential()\n",
        "ml_client = None\n",
        "\n",
        "try:\n",
        "    ml_client = MLClient.from_config(credential=credential)\n",
        "except Exception as ex:\n",
        "    # NOTE: Update following workspace information if not correctly configure before\n",
        "    client_config = {\n",
        "        \"subscription_id\": \"<>\",\n",
        "        \"resource_group\": \"<>\",\n",
        "        \"workspace_name\": \"<>\",\n",
        "    }\n",
        "    if client_config[\"subscription_id\"].startswith(\"<\"):\n",
        "        print(\n",
        "            \"please update your    in notebook cell\"\n",
        "        )\n",
        "        raise ex\n",
        "    else:  # write and reload from config file\n",
        "        import json, os\n",
        "        config_path = \"../azureml/config.json\"\n",
        "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
        "        with open(config_path, \"w\") as fo:\n",
        "            fo.write(json.dumps(client_config))\n",
        "        ml_client = MLClient.from_config(credential=credential, path=config_path)\n",
        "print(ml_client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Configure and run the AutoML classification job\n",
        "In this section we will configure and run the AutoML classification job.\n",
        "\n",
        "## 2.1 Configure the job through the classification() factory function\n",
        "\n",
        "### classification() parameters:\n",
        "\n",
        "The `classification()` factory function allows user to configure AutoML for the classification task for the most common scenarios with the following properties.\n",
        "\n",
        "- `target_column_name` - The name of the column to target for predictions. It must always be specified. This parameter is applicable to 'training_data', 'validation_data' and 'test_data'.\n",
        "- `primary_metric` - The metric that AutoML will optimize for Classification model selection.\n",
        "- `training_data` - The data to be used for training. It should contain both training feature columns and a target column. Optionally, this data can be split for segregating a validation or test dataset. \n",
        "You can use a registered MLTable in the workspace using the format '<mltable_name>:<version>' OR you can use a local file or folder as a MLTable. For e.g Input(mltable='my_mltable:1') OR Input(mltable=MLTable(local_path=\"./data\"))\n",
        "The parameter 'training_data' must always be provided.\n",
        "- `compute` - The compute on which the AutoML job will run. In this example we are using a compute called 'cpu-cluster' present in the workspace. You can replace it any other compute in the workspace. \n",
        "- `name` - The name of the Job/Run. This is an optional property. If not specified, a random name will be generated.\n",
        "- `experiment_name` - The name of the Experiment. An Experiment is like a folder with multiple runs in Azure ML Workspace that should be related to the same logical machine learning experiment.\n",
        "\n",
        "### set_limits() parameters:\n",
        "This is an optional configuration method to configure limits parameters such as timeouts.     \n",
        "    \n",
        "- timeout_minutes - Maximum amount of time in minutes that the whole AutoML job can take before the job terminates. This timeout includes setup, featurization and training runs but does not include the ensembling and model explainability runs at the end of the process since those actions need to happen once all the trials (children jobs) are done. If not specified, the default job's total timeout is 6 days (8,640 minutes). To specify a timeout less than or equal to 1 hour (60 minutes), make sure your dataset's size is not greater than 10,000,000 (rows times column) or an error results.\n",
        "\n",
        "- trial_timeout_minutes - Maximum time in minutes that each trial (child job) can run for before it terminates. If not specified, a value of 1 month or 43200 minutes is used.\n",
        "    \n",
        "- max_trials - The maximum number of trials/runs each with a different combination of algorithm and hyperparameters to try during an AutoML job. If not specified, the default is 1000 trials. If using 'enable_early_termination' the number of trials used can be smaller.\n",
        "    \n",
        "- max_concurrent_trials - Represents the maximum number of trials (children jobs) that would be executed in parallel. It's a good practice to match this number with the number of nodes your cluster.\n",
        "    \n",
        "- enable_early_termination - Whether to enable early termination if the score is not improving in the short term. \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670894317787
        },
        "name": "data-load"
      },
      "outputs": [],
      "source": [
        "# Create MLTables for training dataset\n",
        "named_train_df = ml_client.data.get(name=\"<>\", version=\"5\")\n",
        "named_test_df = ml_client.data.get(name=\"<>\", version=\"5\")\n",
        "\n",
        "# Remote MLTable definition\n",
        "my_training_data_input  = Input(type=AssetTypes.MLTABLE , path=named_train_df.path)# new SDK dataset artefact\n",
        "my_testing_data_input  = Input(type=AssetTypes.MLTABLE, path=named_test_df.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670894318205
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# General job parameters\n",
        "compute_name = \"<>\"\n",
        "max_trials = 5\n",
        "exp_name = \"<>\"\n",
        "target = \"<>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670894319858
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create the AutoML classification job with the related factory-function.\n",
        "\n",
        "classification_job = automl.classification(\n",
        "    compute=compute_name,\n",
        "    experiment_name=exp_name,\n",
        "    training_data=my_training_data_input,\n",
        "    validation_data=my_testing_data_input,\n",
        "    target_column_name=target,\n",
        "    primary_metric=\"accuracy\",\n",
        "    enable_model_explainability=True,\n",
        "    tags={\n",
        "        \"use_case\": \"<>\",\n",
        "        \"label\": \"<>\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Limits are all optional\n",
        "classification_job.set_limits(\n",
        "    timeout_minutes=600,\n",
        "    trial_timeout_minutes=20,\n",
        "    max_trials=max_trials,\n",
        "    # max_concurrent_trials = 4,\n",
        "    # max_cores_per_trial: -1,\n",
        "    enable_early_termination=True,\n",
        ")\n",
        "\n",
        "# Training properties are optional\n",
        "classification_job.set_training(\n",
        "    enable_onnx_compatible_models=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Run the Command\n",
        "Using the `MLClient` created earlier, we will now run this Command in the workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670894325086
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Submit the AutoML job\n",
        "returned_job = ml_client.jobs.create_or_update(\n",
        "    classification_job\n",
        ")  # submit the job to the backend\n",
        "\n",
        "print(f\"Created job: {returned_job}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wait until the AutoML job is finished\n",
        "ml_client.jobs.stream(returned_job.name) waits until the specified job is finished"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670896848683
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ml_client.jobs.stream(returned_job.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670896849552
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(returned_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3.A. Retrieve the Best Trial (Best Model's trial/run)\n",
        "Use the MLFLowClient to access the results (such as Models, Artifacts, Metrics) of a previously completed AutoML Trial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Initialize MLFlow Client\n",
        "The models and artifacts that are produced by AutoML can be accessed via the MLFlow interface. \n",
        "Initialize the MLFlow client here, and set the backend as Azure ML, via. the MLFlow Client.\n",
        "\n",
        "*IMPORTANT*, you need to have installed the latest MLFlow packages with:\n",
        "\n",
        "    pip install azureml-mlflow\n",
        "\n",
        "    pip install mlflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obtain the tracking URI for MLFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670896976560
        }
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "# Obtain the tracking URL from MLClient\n",
        "MLFLOW_TRACKING_URI = ml_client.workspaces.get(\n",
        "    name=ml_client.workspace_name\n",
        ").mlflow_tracking_uri\n",
        "\n",
        "print(MLFLOW_TRACKING_URI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670896996326
        }
      },
      "outputs": [],
      "source": [
        "# Set the MLFLOW TRACKING URI\n",
        "\n",
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "\n",
        "print(\"\\nCurrent tracking uri: {}\".format(mlflow.get_tracking_uri()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670896997973
        }
      },
      "outputs": [],
      "source": [
        "from mlflow.tracking.client import MlflowClient\n",
        "\n",
        "# Initialize MLFlow client\n",
        "mlflow_client = MlflowClient()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get the AutoML parent Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897040470
        }
      },
      "outputs": [],
      "source": [
        "job_name = returned_job.name\n",
        "\n",
        "# Example if providing an specific Job name/ID\n",
        "#job_name = \"<>\"\n",
        "\n",
        "# Get the parent run\n",
        "mlflow_parent_run = mlflow_client.get_run(job_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897042354
        }
      },
      "outputs": [],
      "source": [
        "# Print parent run tags. 'automl_best_child_run_id' tag should be there.\n",
        "print(mlflow_parent_run.data.tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Get the AutoML best child run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897048549
        }
      },
      "outputs": [],
      "source": [
        "# Get the best model's child run\n",
        "\n",
        "#best_child_run_id = mlflow_parent_run.data.tags[\"automl_best_child_run_id\"] # THIS IS THE BEST MODEL!!\n",
        "best_child_run_id = \"<>\" #THIS IS NOT THE BEST MODEL BUT THE BEST TREE BASED MODEL, YOU CAN RETRIEVE IT FROM THE OVERVIEW PAGE OF THE MODEL\n",
        "print(\"Found best child run id: \", best_child_run_id)\n",
        "\n",
        "best_run = mlflow_client.get_run(best_child_run_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Get best model run's metrics\n",
        "\n",
        "Access the results (such as Models, Artifacts, Metrics) of a previously completed AutoML Run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897051914
        }
      },
      "outputs": [],
      "source": [
        "run_metrics = best_run.data.metrics #returns dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 Download the best model locally\n",
        "\n",
        "Access the results (such as Models, Artifacts, Metrics) of a previously completed AutoML Run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897063719
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create local folder\n",
        "local_dir = \"../artifact_downloads\"\n",
        "if not os.path.exists(local_dir):\n",
        "    os.mkdir(local_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897068425
        }
      },
      "outputs": [],
      "source": [
        "# Download run's artifacts/outputs\n",
        "local_path = mlflow_client.download_artifacts(\n",
        "    best_run.info.run_id, \"outputs\", local_dir\n",
        ")\n",
        "print(\"Artifacts downloaded in: {}\".format(local_path))\n",
        "print(\"Artifacts: {}\".format(os.listdir(local_path)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897070523
        }
      },
      "outputs": [],
      "source": [
        "# Show the contents of the MLFlow model folder\n",
        "os.listdir(f\"{local_dir}/outputs/mlflow-model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 3.B. Fit Shap Explainer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897083262
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model_local = joblib.load(f\"{local_dir}/outputs/model.pkl\")\n",
        "\n",
        "#AutoML mapping for feature names after transformation\n",
        "with open(f\"{local_dir}/outputs/engineered_feature_names.json\") as f:\n",
        "    feature_names = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897095359
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "test_df_local = pd.read_csv(\"./data.csv\")\n",
        "X_test = test_df_local.drop(columns=[target])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Prepare data for explainer\n",
        "\n",
        "We will need to apply the pre-processing of the AutoML pipeline before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897103657
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# preprocess the data using the automl transformer object\n",
        "X_test_proc_1 = model_local[0].transform(X_test)\n",
        "X_test_proc = model_local[1].transform(X_test_proc_1)\n",
        "\n",
        "# Grab the model from the AutoML pipeline, we will use it to fit the SHAP explainer\n",
        "model_new_ =model_local[2] #first, isolate the model from the pipeline\n",
        "model_new = model_new_.get_model() #then get the model out of the AutoML wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Fit SHAP explainer object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670460652035
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "explainer = shap.TreeExplainer(model_new) # we are passing no background dataset to increase computational speed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Test explainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897118644
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# WE NEED TO INDEX THE ROW FIRST AND THEN THE PREDICTED VALUE\n",
        "n_row = 0 # ROW INDEX\n",
        "sample_ = X_test.iloc[n_row:n_row+1]\n",
        "\n",
        "predicted_probas = model_local.predict_proba(sample_).T.reset_index() #[0] #PREDICTED VALUE\n",
        "idx = predicted_probas[0].idxmax()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897120170
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# preprocess the data using the automl transformer object\n",
        "X_test_proc_1 = model_local[0].transform(sample_)\n",
        "X_test_proc = model_local[1].transform(X_test_proc_1)\n",
        "shap_values = explainer.shap_values(X_test_proc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897124037
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "shap.initjs() #this is needed to visualise the shap force plot\n",
        "\n",
        "shap.force_plot(explainer.expected_value[idx], shap_values[idx][n_row], feature_names= feature_names, link=\"logit\") #GET EXPLANATION FOR PREDICTED VALUE WITH THE HIGHEST PROBABILITY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Copy explainer and model into the same folder for registration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897138519
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create sub directory within artefacts\n",
        "local_dir_models = \"../artifact_downloads/outputs/models\"\n",
        "if not os.path.exists(local_dir_models):\n",
        "    os.mkdir(local_dir_models)\n",
        "\n",
        "# save the explainer object\n",
        "filename_explainer = f\"{local_dir_models}/explainer.pkl\" #stick to .sav file as there are problems with pickling shap\n",
        "joblib.dump(explainer, filename=filename_explainer)\n",
        "\n",
        "# save the explainer object\n",
        "filename_model = f\"{local_dir_models}/model.pkl\"\n",
        "joblib.dump(model_local, filename=filename_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Register the model and explainer artefacts under the same model register"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897162340
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model_explainer_name = \"test-model-explainer-route-automl\"\n",
        "model = Model(\n",
        "    path=local_dir_models,\n",
        "    name=model_explainer_name,\n",
        "    description=\"my sample mlflow model + explainer\",\n",
        "    type=AssetTypes.CUSTOM_MODEL\n",
        ")\n",
        "\n",
        "registered_model = ml_client.models.create_or_update(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Deploy Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Create managed online endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897167514
        }
      },
      "outputs": [],
      "source": [
        "online_endpoint_name = \"<>\"\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"this is a sample online endpoint for AUTOML model\",\n",
        "    auth_mode=\"key\",\n",
        "    tags={\"type\": \"automl\",\n",
        "    \"use case\": \"<>\",\n",
        "    \"label\": label},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897384276
        }
      },
      "outputs": [],
      "source": [
        "ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Deploy Model & Explainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write scoring file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The scoring script loads both models into a dictionary keyed on their name in the init function. In the run function, each request is parsed for a model key in the JSON to choose the model. The data payload is then passed to the appropriate model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670897384591
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(local_dir_models)\n",
        "os.listdir(local_dir_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Go into the following automl scoring file `../artifact_downloads/output/scoring_file_v_2_0_0.py`, and copy the data sample definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile {local_dir}/outputs/custom_scoring_script_automl_explainer.py\n",
        "\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from azureml.core.model import Model\n",
        "import scipy as sp\n",
        "import shap\n",
        "import sklearn.pipeline\n",
        "from pathlib import Path\n",
        "import os\n",
        "import json\n",
        "\n",
        "from inference_schema.schema_decorators import input_schema, output_schema\n",
        "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
        "from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n",
        "from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n",
        "\n",
        "#COPY FROM AUTOML SCORING FILE scoring_file_v_2_0_0.py\n",
        "data_sample = PandasParameterType({\"\": \"...\"})\n",
        "\n",
        "input_sample = StandardPythonParameterType({'data': data_sample})\n",
        "\n",
        "def init():\n",
        "\n",
        "    global automl_model\n",
        "    global scoring_explainer\n",
        "\n",
        "    # Retrieve the path to the model file using the model name\n",
        "    # Assume original model is named original_prediction_model\n",
        "    model_dir = Path(os.getenv(\"AZUREML_MODEL_DIR\")) / \"models\"\n",
        "    print(os.listdir(model_dir))\n",
        "    automl_model = joblib.load(f\"{model_dir}/model.pkl\")\n",
        "    scoring_explainer = joblib.load(f\"{model_dir}/explainer.pkl\")\n",
        "\n",
        "@input_schema('Inputs', input_sample)\n",
        "def run(Inputs):\n",
        "    ###-----------------\n",
        "    # read data and convert to orginal schema\n",
        "    data = pd.DataFrame(Inputs[\"data\"])\n",
        "\n",
        "    pred_probs = automl_model.predict_proba(data).T.reset_index() #PREDICTED VALUE\n",
        "    pred_probs.columns = [\"class\", \"probability\"]\n",
        "    idx = pred_probs[\"probability\"].idxmax()# INDEX OF THE MAXIMUM PREDICTED \n",
        "    \n",
        "    pred_label  = pred_probs.iloc[idx].astype(\"string\")[0]\n",
        "    pred_probs = pred_probs.to_dict(orient=\"records\")\n",
        "\n",
        "\n",
        "    # PREPARE THE DATA FOR SHAP EXPLAINER\n",
        "    # preprocess the data using the automl transformer object\n",
        "    X_test_proc_1 = automl_model[0].transform(data)\n",
        "    X_test_proc = automl_model[1].transform(X_test_proc_1)\n",
        "    shap_values = scoring_explainer.shap_values(X_test_proc)\n",
        "    shap_values_idx = shap_values[idx][0].astype('float64',casting='same_kind')\n",
        "    base_value = scoring_explainer.expected_value[idx].astype('float64',casting='same_kind')\n",
        "\n",
        "    return {'pred_label': pred_label,\n",
        "    \"pred_probs\":pred_probs, \n",
        "    'shap_values_idx': shap_values_idx.tolist(), \n",
        "    \"Shap base value\" : base_value.tolist()}\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Configure environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670970703491
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Include the correct shap version by checking with current shap version used to fit explainer locally\n",
        "print(shap.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Write a new .yml environment file. Make sure you use Python 3.8 or higher to prevent an error in unpickling the shap explainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile {local_dir}/outputs/conda_env_custom.yml\n",
        "\n",
        "# Conda environment specification. The dependencies defined in this file will\n",
        "# be automatically provisioned for runs with userManagedDependencies=False.\n",
        "\n",
        "# Details about the Conda environment file format:\n",
        "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\n",
        "\n",
        "name: project_environment\n",
        "dependencies:\n",
        "  # The python interpreter version.\n",
        "  # Currently Azure ML only supports 3.8 and later.\n",
        "- python= 3.8.13\n",
        "\n",
        "- pip:\n",
        "  - azureml-train-automl-runtime==1.46.1\n",
        "  - inference-schema\n",
        "  - azureml-interpret==1.46.0\n",
        "  - azureml-defaults==1.46.0\n",
        "- numpy==1.21.6\n",
        "- pandas==1.1.5\n",
        "- scikit-learn==0.22.1\n",
        "- py-xgboost==1.3.3\n",
        "- holidays==0.10.3\n",
        "- psutil==5.9.0\n",
        "- shap==0.39.0\n",
        "- numba==0.55.2\n",
        "channels:\n",
        "- anaconda\n",
        "- conda-forge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670970719524
        }
      },
      "outputs": [],
      "source": [
        "env = Environment(\n",
        "    name=\"automl-tabular-env\",\n",
        "    description=\"environment for automl inference\",\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    conda_file=\"../artifact_downloads/outputs/conda_env_custom.yml\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670971183734
        }
      },
      "outputs": [],
      "source": [
        "code_configuration = CodeConfiguration(\n",
        "        code=f\"{local_dir}/outputs\", scoring_script=\"custom_scoring_script_automl_explainer.py\"\n",
        "        )\n",
        "\n",
        "deployment = ManagedOnlineDeployment(\n",
        "    name=online_endpoint_name, \n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model= registered_model,\n",
        "    environment=env,\n",
        "    code_configuration=code_configuration, #code_configuration,\n",
        "    instance_type=\"Standard_DS2_V2\",\n",
        "    instance_count=1,\n",
        ")\n",
        "\n",
        "ml_client.online_deployments.begin_create_or_update(deployment).result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670971250608
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# automl deployment to take 100% traffic\n",
        "endpoint.traffic = {online_endpoint_name: 100}\n",
        "ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670898364194
        }
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "for i in range(50):\n",
        "\n",
        "    #select samples to predict\n",
        "    tt = X_test.iloc[i:i+1].astype(str)\n",
        "\n",
        "    #define schema to pass into the payload \n",
        "    df_schema = dict(zip(tt.columns, tt.dtypes.values.astype(\"str\")))\n",
        "\n",
        "    #construct the payload object to call the API\n",
        "    payload = {\"Inputs\": {\"data\": tt.to_dict('records')}, \"schema\": df_schema}\n",
        "    #payload = json.dumps(payload)\n",
        "\n",
        "    request_file_name = \"sample-request.json\"\n",
        "\n",
        "    with open(request_file_name, \"w\") as request_file:\n",
        "        json.dump(payload, request_file)\n",
        "\n",
        "    resp = ml_client.online_endpoints.invoke(\n",
        "        endpoint_name=online_endpoint_name,\n",
        "        deployment_name=online_endpoint_name,\n",
        "        request_file=request_file_name)\n",
        "    \n",
        "    print(\"Finished request # \", i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Consume response output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670971459172
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "shap.initjs() #this is needed to visualise the shap force plot\n",
        "\n",
        "shap.force_plot(resp[\"Shap base value\"], np.array(resp[\"shap_values_idx\"]), feature_names= feature_names, link=\"logit\") #GET EXPLANATION FOR PREDICTED VALUE WITH THE HIGHEST PROBABILITY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get endpoint details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670898309019
        }
      },
      "outputs": [],
      "source": [
        "# Get the details for online endpoint\n",
        "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
        "\n",
        "# existing traffic details\n",
        "print(endpoint.traffic)\n",
        "\n",
        "# Get the scoring URI\n",
        "print(endpoint.scoring_uri)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "### Delete the deployment & endpoint\n",
        "ml_client.online_endpoints.begin_delete(name=online_endpoint_name).wait()"
      ]
    }
  ],
  "metadata": {
    "categories": [
      "SDK v2",
      "sdk",
      "python",
      "jobs",
      "automl-standalone-jobs",
      "automl-classification-task-bankmarketing"
    ],
    "interpreter": {
      "hash": "da404a94b19d2e6a57be28cbcf6e71fbd41612916c3423bc5e257c11b3d83fa0"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
